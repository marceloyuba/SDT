{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6163c8-8b11-44e3-9687-60ef14192c17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.6/125.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=2.26.1\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.2/189.2 kB 13.4 MB/s eta 0:00:00\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0\n",
      "  Downloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.3/138.3 kB 17.1 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.6.0\n",
      "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 8.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage) (2.28.1)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.56.4)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 21.6 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.2/181.2 kB 18.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.11)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.3/85.3 kB 10.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyasn1, protobuf, google-crc32c, cachetools, rsa, pyasn1-modules, proto-plus, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Not uninstalling protobuf at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2d0a7ebb-7e1e-460e-9387-68f1d7ae9578\n",
      "    Can't uninstall 'protobuf'. No files were found to uninstall.\n",
      "Successfully installed cachetools-5.3.3 google-api-core-2.18.0 google-auth-2.29.0 google-cloud-core-2.4.1 google-cloud-storage-2.16.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 proto-plus-1.23.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 rsa-4.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%sh pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc3e4a6-6207-4f21-b08a-a86a0f04dc79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6ff7cf-e6a1-472b-9410-1e135523cd87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d751bc8-ab5d-483c-b370-71af04cbe7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo Parquet subido a Cloud Storage: gs://upload-scraping-gcp/import/raw/yellow_tripdata_2024-01.parquet \n",
      "Archivo Parquet subido a Cloud Storage: gs://upload-scraping-gcp/import/raw/green_tripdata_2024-01.parquet \n",
      "Archivo Parquet subido a Cloud Storage: gs://upload-scraping-gcp/import/raw/fhvhv_tripdata_2024-01.parquet \n"
     ]
    }
   ],
   "source": [
    "# Configura la variable de entorno GOOGLE_APPLICATION_CREDENTIALS con la ruta al archivo de credenciales JSON\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Workspace/Users/ccfontanillar@unadvirtual.edu.co/project-test-419903-c2e3452605db.json\"\n",
    "\n",
    "def get_filtered_urls(url):\n",
    "    \"\"\"\n",
    "    Obtiene las URLs filtradas de la página web https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page.\n",
    "\n",
    "    Args:\n",
    "    url (str): URL de la página web.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de URLs filtradas.\n",
    "    \"\"\"\n",
    "    filtered_urls = []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            absolute_url = urljoin(url, href)\n",
    "            # Filtrar los enlaces por los que contienen \"yellow_tripdata\", \"green_tripdata\" o \"fhvhv_tripdata\" y el año 2024\n",
    "            if any(keyword in absolute_url for keyword in [\"yellow_tripdata\", \"green_tripdata\", \"fhvhv_tripdata\"]) and \"2024\" in absolute_url:\n",
    "                filtered_urls.append(absolute_url)\n",
    "\n",
    "    return filtered_urls\n",
    "\n",
    "def process_and_upload_trip_data(filtered_urls):\n",
    "    \"\"\"\n",
    "    Lee cada URL en un DataFrame y lo convierte en formato Parquet antes de subirlo al Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "    filtered_urls (list): Lista de URLs filtradas.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crea un cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Nombre de tu bucket en GCS\n",
    "        bucket_name = 'upload-scraping-gcp'\n",
    "\n",
    "        # Obtiene el bucket\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "        # Recorre las URLs filtradas, lee cada una en un DataFrame y la guarda en un archivo Parquet en GCS\n",
    "        for url in filtered_urls:\n",
    "            try:\n",
    "                # Leer el archivo Parquet en un DataFrame de Pandas\n",
    "                df = pd.read_parquet(url)\n",
    "\n",
    "                # Generar un nombre de archivo único para el Parquet\n",
    "                parquet_file_name = os.path.basename(url)\n",
    "\n",
    "                # Subir el archivo Parquet a GCS\n",
    "                blob = bucket.blob(f\"import/raw/{parquet_file_name}\")\n",
    "                blob.upload_from_string(df.to_parquet(), content_type='application/octet-stream')\n",
    "                print(f\"Archivo Parquet subido a Cloud Storage: gs://{bucket_name}/import/raw/{parquet_file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar {url}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en el proceso de carga a Cloud Storage: {e}\")\n",
    "\n",
    "# URL del sitio web que deseas scrape\n",
    "url = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "\n",
    "# Obtiene las URLs filtradas\n",
    "filtered_urls = get_filtered_urls(url)\n",
    "\n",
    "# Procesa y sube los datos de cada URL\n",
    "process_and_upload_trip_data(filtered_urls)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2393719124911909,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "WebScraping_NYC_GCP_DataUpload",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
